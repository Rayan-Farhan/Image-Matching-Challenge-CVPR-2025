{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352b076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:18:26.286030Z",
     "iopub.status.busy": "2025-12-07T22:18:26.285630Z",
     "iopub.status.idle": "2025-12-07T22:26:53.377642Z",
     "shell.execute_reply": "2025-12-07T22:26:53.376447Z"
    },
    "papermill": {
     "duration": 507.096961,
     "end_time": "2025-12-07T22:26:53.379237",
     "exception": false,
     "start_time": "2025-12-07T22:18:26.282276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ \"stairs\": 39/51 images, 1 clusters\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "✓ \"ETs\": 20/22 images, 1 clusters\n",
      "✗ \"amy_gardens\": Failed!\n",
      "✗ \"fbk_vineyard\": Failed!\n",
      "✗ \"imc2023_haiper\": Failed!\n",
      "✗ \"imc2023_heritage\": Failed!\n",
      "✗ \"imc2023_theather_imc2024_church\": Failed!\n",
      "✗ \"imc2024_dioscuri_baalshamin\": Failed!\n",
      "✗ \"imc2024_lizard_pond\": Failed!\n",
      "✗ \"pt_brandenburg_british_buckingham\": Failed!\n",
      "✗ \"pt_piazzasanmarco_grandplace\": Failed!\n",
      "✗ \"pt_sacrecoeur_trevi_tajmahal\": Failed!\n",
      "✗ \"pt_stpeters_stpauls\": Failed!\n",
      "✓ \"stairs\": 39/51 images, 1 clusters\n",
      "\n",
      "======================================================================\n",
      "TIMING SUMMARY\n",
      "======================================================================\n",
      "shortlisting        :   10.64s\n",
      "feature_detection   :    9.26s\n",
      "feature_matching    :  313.84s\n",
      "RANSAC              :    1.78s\n",
      "Reconstruction      :   98.54s\n",
      "\n",
      "✓ Submission saved to: /kaggle/working/submission.csv\n",
      "\n",
      "======================================================================\n",
      "SUBMISSION VALIDATION\n",
      "======================================================================\n",
      "image_id,dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "ETs_another_et_another_et001.png_public,ETs,cluster0,another_et_another_et001.png,0.999776171;0.002971523;-0.020947008;-0.006434279;0.985894798;-0.167242481;0.020154582;0.167339826;0.985693246,-2.069570006;-1.513031927;1.440725942\r\n",
      "ETs_another_et_another_et002.png_public,ETs,cluster0,another_et_another_et002.png,0.999348040;0.001950588;-0.036051225;-0.008749917;0.981856138;-0.189425350;0.035027626;0.189617297;0.981233074,-1.905143346;-0.753584971;0.039620422\r\n",
      "ETs_another_et_another_et003.png_public,ETs,cluster0,another_et_another_et003.png,0.999378490;-0.027952551;0.021477630;0.032385415;0.968669383;-0.246233245;-0.013921875;0.246775771;0.968972600,-2.170605742;0.763987833;-1.614726448\r\n",
      "ETs_another_et_another_et004.png_public,ETs,cluster0,another_et_another_et004.png,0.999997682;-0.000441087;0.002107667;0.000559825;0.998392686;-0.056672133;-0.002079282;0.056673181;0.998390619,-2.056688914;-1.151753148;-1.246915327\r\n",
      "ETs_another_et_another_et005.png_public,ETs,cluster0,another_et_another_et005.png,0.998005271;0.024598887;0.058140981;-0.017213265;0.992098253;-0.124276949;-0.060738640;0.123028254;0.990542713,-2.618132713;-1.738484932;0.072443246\r\n",
      "ETs_another_et_another_et006.png_public,ETs,cluster0,another_et_another_et006.png,0.906898620;0.131017994;-0.400461206;-0.231285190;0.949234065;-0.213217847;0.352196044;0.285987717;0.891161586,0.436647412;-0.448694730;0.224959513\r\n",
      "ETs_another_et_another_et007.png_public,ETs,cluster0,another_et_another_et007.png,0.760151443;0.158415254;-0.630138391;-0.321137063;0.934687057;-0.152417494;0.564836942;0.318221171;0.761376724,2.229417724;-0.287389202;0.072864203\r\n",
      "ETs_another_et_another_et008.png_public,ETs,cluster0,another_et_another_et008.png,0.542635131;0.167590829;-0.823079843;-0.401732035;0.912335816;-0.079086859;0.737670988;0.373572849;0.562392070,3.922398336;-0.722159762;1.156447664\r\n",
      "ETs_another_et_another_et009.png_public,ETs,cluster0,another_et_another_et009.png,0.291840270;0.179323428;-0.939506447;-0.494869780;0.868882601;0.012121339;0.818494445;0.461395854;0.342316679,5.472467969;-1.192294450;2.085379952\r\n",
      "ETs_another_et_another_et010.png_public,ETs,cluster0,another_et_another_et010.png,-0.215921998;0.184722257;-0.958778065;-0.563472631;0.778361968;0.276859605;0.797418513;0.600025278;-0.063979534,6.365134764;-2.541047205;4.519294625\r\n",
      "ETs_et_et000.png_public,ETs,cluster0,et_et000.png,0.621726201;0.450130708;-0.640967142;-0.510218191;0.853660912;0.104596584;0.594250729;0.262002658;0.760408231,3.001508796;-0.826102684;2.009690161\r\n",
      "ETs_et_et001.png_public,ETs,cluster0,et_et001.png,0.739710988;0.518829575;-0.428536494;-0.508485654;0.848072478;0.149048352;0.440760700;0.107651956;0.891145926,1.267498362;-1.015675570;2.491271656\r\n",
      "ETs_et_et002.png_public,ETs,cluster0,et_et002.png,0.878863786;0.392460223;-0.271244205;-0.379899504;0.919639609;0.099696322;0.288573755;0.015426052;0.957333393,0.296564692;-0.709867881;2.630930776\r\n",
      "ETs_et_et003.png_public,ETs,cluster0,et_et003.png,0.592791488;0.388307632;-0.705560370;-0.453225592;0.885035516;0.106295331;0.665721274;0.256767049;0.700632477,3.287512909;-0.877252946;2.298052622\r\n",
      "ETs_et_et004.png_public,ETs,cluster0,et_et004.png,0.857330358;0.292863350;-0.423338771;-0.430845187;0.858287673;-0.278773557;0.281703891;0.421394505;0.862014843,1.208310324;1.879334965;1.613181359\r\n",
      "ETs_et_et005.png_public,ETs,cluster0,et_et005.png,0.900108416;0.060578873;0.431433703;0.005345739;0.988675229;-0.149975711;-0.435633175;0.137300732;0.889591056,-3.901410218;-0.002612185;2.521319887\r\n",
      "ETs_et_et006.png_public,ETs,cluster0,et_et006.png,0.908814266;0.146192757;0.390748396;-0.168457308;0.985437866;0.023115967;-0.381678878;-0.086832544;0.920207228,-3.711940609;-0.784788073;2.733658722\r\n",
      "ETs_et_et007.png_public,ETs,cluster0,et_et007.png,0.893444691;0.093931893;0.439241828;-0.099262461;0.995001855;-0.010875306;-0.438067972;-0.033883741;0.898303036,-4.463486500;-0.335715410;3.534018566\r\n",
      "ETs_et_et008.png_public,ETs,cluster0,et_et008.png,0.774074938;-0.169238540;0.610054348;0.215554264;0.976488360;-0.002615650;-0.595268301;0.133524525;0.792355256,-5.715179406;-1.825223809;4.062898726\r\n",
      "\n",
      "✓ Total rows: 1945\n",
      "✓ Columns: ['image_id', 'dataset', 'scene', 'image', 'rotation_matrix', 'translation_vector']\n",
      "✓ All required columns present\n",
      "\n",
      "✓ Registered images: 59\n",
      "✓ Outlier images: 1886\n",
      "✓ Registration rate: 3.0%\n",
      "\n",
      "✓ Total clusters: 2\n",
      "\n",
      "Cluster sizes:\n",
      "  pt_brandenburg_british_buckingham/outliers: 225 images\n",
      "  pt_sacrecoeur_trevi_tajmahal/outliers: 225 images\n",
      "  imc2024_lizard_pond/outliers: 214 images\n",
      "  imc2023_heritage/outliers: 209 images\n",
      "  amy_gardens/outliers: 200 images\n",
      "  pt_stpeters_stpauls/outliers: 200 images\n",
      "  pt_piazzasanmarco_grandplace/outliers: 168 images\n",
      "  fbk_vineyard/outliers: 163 images\n",
      "  imc2024_dioscuri_baalshamin/outliers: 138 images\n",
      "  imc2023_theather_imc2024_church/outliers: 76 images\n",
      "\n",
      "======================================================================\n",
      "✓ SUBMISSION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index /kaggle/input/imc2025-packages-python-11-new/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    \"\"\"All tunable hyperparameters in one place\"\"\"\n",
    "    \n",
    "    sim_threshold: float = 0.40\n",
    "    \n",
    "    min_pairs_per_image: int = 25\n",
    "    \n",
    "    exhaustive_threshold: int = 20\n",
    "    \n",
    "    num_features: int = 6000\n",
    "    \n",
    "    detection_threshold: float = 0.01\n",
    "    \n",
    "    resize_to: int = 1600\n",
    "    \n",
    "    min_matches: int = 20\n",
    "    \n",
    "    min_model_size: int = 3\n",
    "    \n",
    "    max_num_models: int = 30\n",
    "    \n",
    "    def get_for_dataset(self, dataset_name: str, num_images: int):\n",
    "        \"\"\"Apply adaptive parameters based on dataset characteristics\"\"\"\n",
    "        config = dataclasses.replace(self)\n",
    "        \n",
    "        if num_images <= 30:\n",
    "            # Small scenes - can afford more features and higher resolution\n",
    "            config.num_features = 8000\n",
    "            config.resize_to = 2048\n",
    "            config.exhaustive_threshold = 30\n",
    "            config.detection_threshold = 0.005\n",
    "            \n",
    "        elif num_images >= 200:\n",
    "            # Large scenes - need more connectivity\n",
    "            config.sim_threshold = 0.35\n",
    "            config.min_pairs_per_image = 35\n",
    "            config.max_num_models = 50\n",
    "        \n",
    "        # Dataset-specific tweaks (add your observations here)\n",
    "        if 'vineyard' in dataset_name.lower():\n",
    "            # Repetitive structures - be more strict\n",
    "            config.detection_threshold = 0.02\n",
    "            config.min_matches = 25\n",
    "            \n",
    "        elif 'brandenburg' in dataset_name.lower() or 'church' in dataset_name.lower():\n",
    "            # Architectural scenes with detail\n",
    "            config.num_features = 7000\n",
    "            config.resize_to = 2048\n",
    "        \n",
    "        return config\n",
    "\n",
    "CONFIG = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device=}')\n",
    "\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "def get_global_desc(fnames, device=torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval().to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    \n",
    "    for i, img_fname_full in tqdm(enumerate(fnames), total=len(fnames)):\n",
    "        timg = load_torch_image(img_fname_full, device='cpu')\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:, 1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    \n",
    "    return torch.cat(global_descs_dinov2, dim=0)\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i + 1, len(img_fnames)):\n",
    "            index_pairs.append((i, j))\n",
    "    return index_pairs\n",
    "\n",
    "def get_image_pairs_shortlist(fnames, sim_th, min_pairs, exhaustive_if_less, device):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    \n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    \n",
    "    mask = dm <= sim_th\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    \n",
    "    for st_idx in range(num_imgs - 1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "        \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < 1000:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "    \n",
    "    return sorted(list(set(matching_list)))\n",
    "\n",
    "def detect_aliked(img_fnames, feature_dir, num_features, detection_threshold, resize_to, device):\n",
    "    dtype = torch.float32\n",
    "    extractor = ALIKED(\n",
    "        max_num_keypoints=num_features,\n",
    "        detection_threshold=detection_threshold,\n",
    "        resize=resize_to\n",
    "    ).eval().to(device, dtype)\n",
    "    \n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        \n",
    "        for img_path in tqdm(img_fnames):\n",
    "            key = img_path.split('/')[-1]\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                image = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats = extractor.extract(image)\n",
    "                \n",
    "                kpts = feats['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = feats['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n",
    "                \n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "\n",
    "def match_with_lightglue(img_fnames, index_pairs, feature_dir, min_matches, device, verbose=True):\n",
    "    lg_matcher = KF.LightGlueMatcher(\n",
    "        \"aliked\",\n",
    "        {\"width_confidence\": -1, \"depth_confidence\": -1, \"mp\": 'cuda' in str(device)}\n",
    "    ).eval().to(device)\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "         h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        \n",
    "        for idx1, idx2 in tqdm(index_pairs):\n",
    "            key1 = img_fnames[idx1].split('/')[-1]\n",
    "            key2 = img_fnames[idx2].split('/')[-1]\n",
    "            \n",
    "            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
    "            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
    "            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                laf1 = KF.laf_from_center_scale_ori(kp1[None])\n",
    "                laf2 = KF.laf_from_center_scale_ori(kp2[None])\n",
    "                dists, idxs = lg_matcher(desc1, desc2, laf1, laf2)\n",
    "            \n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "            \n",
    "            n_matches = len(idxs)\n",
    "            if verbose:\n",
    "                print(f'{key1}-{key2}: {n_matches} matches')\n",
    "            \n",
    "            if n_matches >= min_matches:\n",
    "                group = f_match.require_group(key1)\n",
    "                group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir, database_path):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', False)\n",
    "    add_matches(db, feature_dir, fname_to_id)\n",
    "    db.commit()\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Configuration\n",
    "is_train = False\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "sample_submission_csv = os.path.join(data_dir, 'train_labels.csv' if is_train else 'sample_submission.csv')\n",
    "\n",
    "# Load datasets\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Processing configuration\n",
    "max_images = None\n",
    "datasets_to_process = None\n",
    "if is_train:\n",
    "    datasets_to_process = ['amy_gardens', 'ETs', 'fbk_vineyard', 'stairs']\n",
    "\n",
    "# Processing\n",
    "timings = {\n",
    "    \"shortlisting\": [], \"feature_detection\": [], \"feature_matching\": [],\n",
    "    \"RANSAC\": [], \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "print(f\"\\nExtracting on device {device}\")\n",
    "print(f\"\\nDefault hyperparameters:\")\n",
    "print(f\"  sim_threshold: {CONFIG.sim_threshold}\")\n",
    "print(f\"  min_pairs_per_image: {CONFIG.min_pairs_per_image}\")\n",
    "print(f\"  num_features: {CONFIG.num_features}\")\n",
    "print(f\"  detection_threshold: {CONFIG.detection_threshold}\")\n",
    "print(f\"  resize_to: {CONFIG.resize_to}\")\n",
    "print(f\"  min_matches: {CONFIG.min_matches}\\n\")\n",
    "\n",
    "for dataset, predictions in samples.items():\n",
    "    if datasets_to_process and dataset not in datasets_to_process:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "    images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "    if max_images:\n",
    "        images = images[:max_images]\n",
    "    \n",
    "    num_images = len(images)\n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    print(f'Processing dataset \"{dataset}\": {num_images} images')\n",
    "    print(f'{\"=\"*70}')\n",
    "    \n",
    "    # Get adaptive config for this dataset\n",
    "    cfg = CONFIG.get_for_dataset(dataset, num_images)\n",
    "    print(f\"Adaptive params: features={cfg.num_features}, resize={cfg.resize_to}, \"\n",
    "          f\"sim_th={cfg.sim_threshold:.2f}, min_pairs={cfg.min_pairs_per_image}\")\n",
    "    \n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "    feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Shortlisting\n",
    "        t = time()\n",
    "        index_pairs = get_image_pairs_shortlist(\n",
    "            images, cfg.sim_threshold, cfg.min_pairs_per_image,\n",
    "            cfg.exhaustive_threshold, device\n",
    "        )\n",
    "        timings['shortlisting'].append(time() - t)\n",
    "        print(f'✓ Shortlisting: {len(index_pairs)} pairs in {time() - t:.2f}s')\n",
    "        gc.collect()\n",
    "        \n",
    "        # Feature detection\n",
    "        t = time()\n",
    "        detect_aliked(images, feature_dir, cfg.num_features, \n",
    "                     cfg.detection_threshold, cfg.resize_to, device)\n",
    "        gc.collect()\n",
    "        timings['feature_detection'].append(time() - t)\n",
    "        print(f'✓ Feature detection in {time() - t:.2f}s')\n",
    "        \n",
    "        # Feature matching\n",
    "        t = time()\n",
    "        match_with_lightglue(images, index_pairs, feature_dir, \n",
    "                           cfg.min_matches, device, verbose=False)\n",
    "        timings['feature_matching'].append(time() - t)\n",
    "        print(f'✓ Feature matching in {time() - t:.2f}s')\n",
    "        \n",
    "        # Import to COLMAP\n",
    "        database_path = os.path.join(feature_dir, 'colmap.db')\n",
    "        if os.path.isfile(database_path):\n",
    "            os.remove(database_path)\n",
    "        gc.collect()\n",
    "        sleep(1)\n",
    "        import_into_colmap(images_dir, feature_dir, database_path)\n",
    "        output_path = f'{feature_dir}/colmap_rec_aliked'\n",
    "        \n",
    "        # RANSAC\n",
    "        t = time()\n",
    "        pycolmap.match_exhaustive(database_path)\n",
    "        timings['RANSAC'].append(time() - t)\n",
    "        print(f'✓ RANSAC in {time() - t:.2f}s')\n",
    "        \n",
    "        # Reconstruction\n",
    "        mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "        mapper_options.min_model_size = cfg.min_model_size\n",
    "        mapper_options.max_num_models = cfg.max_num_models\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        t = time()\n",
    "        maps = pycolmap.incremental_mapping(\n",
    "            database_path=database_path,\n",
    "            image_path=images_dir,\n",
    "            output_path=output_path,\n",
    "            options=mapper_options\n",
    "        )\n",
    "        sleep(1)\n",
    "        timings['Reconstruction'].append(time() - t)\n",
    "        print(f'✓ Reconstruction in {time() - t:.2f}s')\n",
    "        clear_output(wait=False)\n",
    "        \n",
    "        # Store results\n",
    "        registered = 0\n",
    "        for map_index, cur_map in maps.items():\n",
    "            for index, image in cur_map.images.items():\n",
    "                prediction_index = filename_to_index[image.name]\n",
    "                predictions[prediction_index].cluster_index = map_index\n",
    "                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                registered += 1\n",
    "        \n",
    "        result = f'✓ \"{dataset}\": {registered}/{num_images} images, {len(maps)} clusters'\n",
    "        mapping_result_strs.append(result)\n",
    "        print(result)\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        mapping_result_strs.append(f'✗ \"{dataset}\": Failed!')\n",
    "\n",
    "# Results\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('FINAL RESULTS')\n",
    "print(f'{\"=\"*70}')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('TIMING SUMMARY')\n",
    "print(f'{\"=\"*70}')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k:20s}: {sum(v):7.2f}s')\n",
    "\n",
    "# Create submission\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        # Training format: dataset,scene,image,rotation_matrix,translation_vector\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rot = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                trans = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster},{prediction.filename},{rot},{trans}\\n')\n",
    "    else:\n",
    "        # Test format: image_id,dataset,scene,image,rotation_matrix,translation_vector\n",
    "        # IMPORTANT: image_id must be from sample_submission.csv\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                # Use cluster name or 'outliers' for unregistered images\n",
    "                cluster = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                \n",
    "                # Format rotation and translation with nan for outliers\n",
    "                rot = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                trans = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                \n",
    "                # Write with image_id (critical for test submission)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster},{prediction.filename},{rot},{trans}\\n')\n",
    "\n",
    "print(f'\\n✓ Submission saved to: {submission_file}')\n",
    "\n",
    "# Validate submission format\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUBMISSION VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check file exists and show first few lines\n",
    "!head -20 {submission_file}\n",
    "\n",
    "# Validate format\n",
    "import csv\n",
    "with open(submission_file, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    rows = list(reader)\n",
    "    \n",
    "    print(f\"\\n✓ Total rows: {len(rows)}\")\n",
    "    \n",
    "    if is_train:\n",
    "        required_cols = ['dataset', 'scene', 'image', 'rotation_matrix', 'translation_vector']\n",
    "    else:\n",
    "        required_cols = ['image_id', 'dataset', 'scene', 'image', 'rotation_matrix', 'translation_vector']\n",
    "    \n",
    "    actual_cols = reader.fieldnames\n",
    "    print(f\"✓ Columns: {actual_cols}\")\n",
    "    \n",
    "    # Check all required columns present\n",
    "    missing_cols = set(required_cols) - set(actual_cols)\n",
    "    if missing_cols:\n",
    "        print(f\"✗ ERROR: Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"✓ All required columns present\")\n",
    "    \n",
    "    # Check format of some rows\n",
    "    outlier_count = 0\n",
    "    registered_count = 0\n",
    "    \n",
    "    for row in rows:\n",
    "        if row['scene'] == 'outliers':\n",
    "            outlier_count += 1\n",
    "            # Verify outliers have nan values\n",
    "            rot_parts = row['rotation_matrix'].split(';')\n",
    "            trans_parts = row['translation_vector'].split(';')\n",
    "            if len(rot_parts) != 9 or len(trans_parts) != 3:\n",
    "                print(f\"✗ ERROR: Outlier {row['image']} has wrong number of values\")\n",
    "            if not all(p == 'nan' for p in rot_parts + trans_parts):\n",
    "                print(f\"✗ WARNING: Outlier {row['image']} should have all nan values\")\n",
    "        else:\n",
    "            registered_count += 1\n",
    "            # Verify registered images have numeric values\n",
    "            rot_parts = row['rotation_matrix'].split(';')\n",
    "            trans_parts = row['translation_vector'].split(';')\n",
    "            if len(rot_parts) != 9:\n",
    "                print(f\"✗ ERROR: Image {row['image']} rotation has {len(rot_parts)} values (need 9)\")\n",
    "            if len(trans_parts) != 3:\n",
    "                print(f\"✗ ERROR: Image {row['image']} translation has {len(trans_parts)} values (need 3)\")\n",
    "    \n",
    "    print(f\"\\n✓ Registered images: {registered_count}\")\n",
    "    print(f\"✓ Outlier images: {outlier_count}\")\n",
    "    print(f\"✓ Registration rate: {100*registered_count/len(rows):.1f}%\")\n",
    "    \n",
    "    # Check cluster distribution\n",
    "    clusters = {}\n",
    "    for row in rows:\n",
    "        dataset = row['dataset']\n",
    "        scene = row['scene']\n",
    "        key = f\"{dataset}/{scene}\"\n",
    "        clusters[key] = clusters.get(key, 0) + 1\n",
    "    \n",
    "    print(f\"\\n✓ Total clusters: {len([k for k in clusters if 'outliers' not in k])}\")\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    for cluster, count in sorted(clusters.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"  {cluster}: {count} images\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ SUBMISSION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "isSourceIdPinned": false,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 7305232,
     "sourceId": 11660458,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11924468,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 515.762386,
   "end_time": "2025-12-07T22:26:57.053976",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-07T22:18:21.291590",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
